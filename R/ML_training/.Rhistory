rho_DL <- seq(.75, .999, .000001)
distribution_DL <- "gaussian"
epochs_DL <- 1:10000
mini_batch_size_DL <- 1:nrow(train.dataset)
fast_mode_DL <- FALSE
reproducible_DL <- TRUE # running with FALSE didn't converge (didn't want to wait to see if it would)
hidden_dropout_DL <- seq(0, .33, .01)
# Cannot grid mini-batch-size
activation_function_DL <- "RectifierWithDropout"
nfolds_DL = 5
max_runtime_secs_DL = 0 # the time each model will at most run in seconds
train_DL = TRUE
# total number of models
number_of_models = c(number_of_GLM_models, number_of_RF_models,
number_of_GBM_models, number_of_DL_models) # GLM, RF, GBM, DL
total_number_of_models = sum(number_of_models)
# dataset for all models
nVariblesToPredict = length(X.list)
MSE_all_dataset = data.frame(algorithm = rep( c( rep("GLM", number_of_GLM_models), rep("RF", number_of_RF_models), rep("GBM", number_of_GBM_models), rep("DL", number_of_DL_models) ), nVariblesToPredict),
model = rep(c(1:number_of_GLM_models, 1:number_of_RF_models, 1:number_of_GBM_models, 1:number_of_DL_models), nVariblesToPredict),
MSE_CV = rep(Inf, total_number_of_models*nVariblesToPredict),
MSE_training = rep(Inf, total_number_of_models*nVariblesToPredict),
MSE_testing = rep(Inf, total_number_of_models*nVariblesToPredict),
MSE_mean = rep(Inf, total_number_of_models*nVariblesToPredict),
error_training_mean = rep(Inf, total_number_of_models*nVariblesToPredict),
error_training_SEM = rep(Inf, total_number_of_models*nVariblesToPredict),
error_training_max = rep(Inf, total_number_of_models*nVariblesToPredict),
error_training_percent_mean = rep(Inf, total_number_of_models*nVariblesToPredict),
error_training_percent_SEM = rep(Inf, total_number_of_models*nVariblesToPredict),
error_training_percent_max = rep(Inf, total_number_of_models*nVariblesToPredict),
error_testing_mean = rep(Inf, total_number_of_models*nVariblesToPredict),
error_testing_SEM = rep(Inf, total_number_of_models*nVariblesToPredict),
error_testing_max = rep(Inf, total_number_of_models*nVariblesToPredict),
error_testing_percent_mean = rep(Inf, total_number_of_models*nVariblesToPredict),
error_testing_percent_SEM = rep(Inf, total_number_of_models*nVariblesToPredict),
error_testing_percent_max = rep(Inf, total_number_of_models*nVariblesToPredict),
error_mean_mean = rep(Inf, total_number_of_models*nVariblesToPredict),
error_mean_SEM = rep(Inf, total_number_of_models*nVariblesToPredict),
error_mean_max = rep(Inf, total_number_of_models*nVariblesToPredict),
error_mean_percent_mean = rep(Inf, total_number_of_models*nVariblesToPredict),
error_mean_percent_SEM = rep(Inf, total_number_of_models*nVariblesToPredict),
error_mean_percent_max = rep(Inf, total_number_of_models*nVariblesToPredict))
# only for the best models + times and other stuffs. Best model is defined as the onw that minimizes MSE_CV
MSE_best_dataset = data.frame(algorithm = rep( c("GLM", "RF", "GBM", "DL"), nVariblesToPredict),
model = rep(0, 4*nVariblesToPredict),
MSE_CV = rep(Inf, 4*nVariblesToPredict),
MSE_training = rep(Inf, 4*nVariblesToPredict),
MSE_testing = rep(Inf, 4*nVariblesToPredict),
MSE_mean = rep(Inf, 4*nVariblesToPredict),
error_training_mean = rep(Inf, 4*nVariblesToPredict),
error_training_SEM = rep(Inf, 4*nVariblesToPredict),
error_training_max = rep(Inf, 4*nVariblesToPredict),
error_training_percent_mean = rep(Inf, 4*nVariblesToPredict),
error_training_percent_SEM = rep(Inf, 4*nVariblesToPredict),
error_training_percent_max = rep(Inf, 4*nVariblesToPredict),
error_testing_mean = rep(Inf, 4*nVariblesToPredict),
error_testing_SEM = rep(Inf, 4*nVariblesToPredict),
error_testing_max = rep(Inf, 4*nVariblesToPredict),
error_testing_percent_mean = rep(Inf, 4*nVariblesToPredict),
error_testing_percent_SEM = rep(Inf, 4*nVariblesToPredict),
error_testing_percent_max = rep(Inf, 4*nVariblesToPredict),
error_mean_mean = rep(Inf, 4*nVariblesToPredict),
error_mean_SEM = rep(Inf, 4*nVariblesToPredict),
error_mean_max = rep(Inf, 4*nVariblesToPredict),
error_mean_percent_mean = rep(Inf, 4*nVariblesToPredict),
error_mean_percent_SEM = rep(Inf, 4*nVariblesToPredict),
error_mean_percent_max = rep(Inf, 4*nVariblesToPredict),
algorithm_run_time = rep(Inf, 4*nVariblesToPredict),
time_performance_CV = rep(Inf, 4*nVariblesToPredict),
time_prediction_training = rep(Inf, 4*nVariblesToPredict),
time_performance_training = rep(Inf, 4*nVariblesToPredict),
time_prediction_testing = rep(Inf, 4*nVariblesToPredict),
time_performance_testing = rep(Inf, 4*nVariblesToPredict),
time_prediction_mean = rep(Inf, 4*nVariblesToPredict),
time_performance_mean = rep(Inf, 4*nVariblesToPredict),
time_errors = rep(Inf, 4*nVariblesToPredict),
n_models = rep(0, 4*nVariblesToPredict),
n_training = rep(nrow(train.dataset), 4*nVariblesToPredict),
n_testing = rep(nrow(test.dataset), 4*nVariblesToPredict),
n_mean = rep(nrow(mean.dataset), 4*nVariblesToPredict))
it = 1
Y <- ys[it] # variable to predict
X <- X.list[[it]] # variable to predict
interactions_GLM = interaction_list[[it]]
# updating parameters for RF
mtries_RF <- 1:length(X)
algo = 1
# GLM
print("begin penalized linear regression")
modelGrid <- h2o.grid(algorithm = "glm", x=X, y=Y, training_frame = train.h2o,
validation_frame = test.h2o, family = family_GLM,
hyper_params = list(alpha = alpha_GLM, lambda = lambda_GLM),
search_criteria = list(strategy = "RandomDiscrete",
max_models = number_of_GLM_models,
seed = seeds_GLM[1]),
keep_cross_validation_predictions = TRUE,
lambda_search = lambda_search_GLM,
max_iterations = max_iterations_GLM,
max_runtime_secs = max_runtime_secs_GLM,
nfolds = nfolds_GLM,
fold_assignment = "Modulo",
nlambdas = nlambdas_GLM,
remove_collinear_columns=TRUE,
interactions = interactions_GLM)
model_name = "penalized linear models"
folder_root = "GLM"
print("finished penalized linear regression")
algorithm_run_time = difftime(Sys.time(), algorithm_run_time_i, units = "sec")
print(paste0("saving ", model_name))
number_of_cores
# investigating the interactions for GLM and creating LM models
library(lme4)
##### Ta_brooder
LM.Ta_brooder_1 = lm(Ta_brooder ~ Ta_pen*Heat_source_power, data = train.dataset)
# summary(LM.Ta_brooder_1)
# anova(LM.Ta_brooder_1) # suggests removing interaction (p = 0.1304)
LM.Ta_brooder_2 = lm(Ta_brooder ~ Ta_pen + Heat_source_power, data = train.dataset)
# summary(LM.Ta_brooder_2)
# anova(LM.Ta_brooder_2)
# anova(LM.Ta_brooder_2, LM.Ta_brooder_1) # no difference (p = 0.1304)
LM.Ta_brooder_final = LM.Ta_brooder_2
# cross-validation
LM.Ta_brooder_final_CV1 = update(LM.Ta_brooder_final, data = train.dataset[ -seq(1, nrow(train.dataset), by = 5) ,] )
MSE.LM.Ta_brooder_final = sum( (predict(LM.Ta_brooder_final_CV1, train.dataset[seq(1, nrow(train.dataset), by = 5) ,]) - train.dataset$Ta_brooder[seq(1, nrow(train.dataset), by = 5)])^2 )/nrow(train.dataset)
LM.Ta_brooder_final_CV2 = update(LM.Ta_brooder_final, data = train.dataset[ -seq(2, nrow(train.dataset), by = 5) ,] )
MSE.LM.Ta_brooder_final = sum( (predict(LM.Ta_brooder_final_CV2, train.dataset[seq(2, nrow(train.dataset), by = 5) ,]) - train.dataset$Ta_brooder[seq(2, nrow(train.dataset), by = 5)])^2 )/nrow(train.dataset) + MSE.LM.Ta_brooder_final
LM.Ta_brooder_final_CV3 = update(LM.Ta_brooder_final, data = train.dataset[ -seq(3, nrow(train.dataset), by = 5) ,] )
MSE.LM.Ta_brooder_final = sum( (predict(LM.Ta_brooder_final_CV3, train.dataset[seq(3, nrow(train.dataset), by = 5) ,]) - train.dataset$Ta_brooder[seq(3, nrow(train.dataset), by = 5)])^2 )/nrow(train.dataset) + MSE.LM.Ta_brooder_final
LM.Ta_brooder_final_CV4 = update(LM.Ta_brooder_final, data = train.dataset[ -seq(4, nrow(train.dataset), by = 5) ,] )
MSE.LM.Ta_brooder_final = sum( (predict(LM.Ta_brooder_final_CV4, train.dataset[seq(4, nrow(train.dataset), by = 5) ,]) - train.dataset$Ta_brooder[seq(4, nrow(train.dataset), by = 5)])^2 )/nrow(train.dataset) + MSE.LM.Ta_brooder_final
LM.Ta_brooder_final_CV5 = update(LM.Ta_brooder_final, data = train.dataset[ -seq(5, nrow(train.dataset), by = 5) ,] )
MSE.LM.Ta_brooder_final = sum( (predict(LM.Ta_brooder_final_CV5, train.dataset[seq(5, nrow(train.dataset), by = 5) ,]) - train.dataset$Ta_brooder[seq(5, nrow(train.dataset), by = 5)])^2 )/nrow(train.dataset) + MSE.LM.Ta_brooder_final
# retraining the model with all datapoints
LM.Ta_brooder_final_all = update(LM.Ta_brooder_final, data = all.dataset)
##### Tg_brooder
LM.Tg_brooder_1 = lm(Tg_brooder ~ Ta_brooder*Heat_source_power, data = train.dataset)
# summary(LM.Tg_brooder_1)
# anova(LM.Tg_brooder_1) # weak suggestion to remove interaction (p = 0.0549). Will remove it
LM.Tg_brooder_2 = lm(Tg_brooder ~ Ta_brooder + Heat_source_power, data = train.dataset)
# summary(LM.Tg_brooder_2)
# anova(LM.Tg_brooder_2, LM.Tg_brooder_1) # not different (p = 0.0549)
# anova (LM.Tg_brooder_2) # everything is significat
LM.Tg_brooder_final = LM.Tg_brooder_2
# cross-validation
LM.Tg_brooder_final_CV1 = update(LM.Tg_brooder_final, data = train.dataset[ -seq(1, nrow(train.dataset), by = 5) ,] )
MSE.LM.Tg_brooder_final = sum( (predict(LM.Tg_brooder_final_CV1, train.dataset[seq(1, nrow(train.dataset), by = 5) ,]) - train.dataset$Tg_brooder[seq(1, nrow(train.dataset), by = 5)])^2 )/nrow(train.dataset)
LM.Tg_brooder_final_CV2 = update(LM.Tg_brooder_final, data = train.dataset[ -seq(2, nrow(train.dataset), by = 5) ,] )
MSE.LM.Tg_brooder_final = sum( (predict(LM.Tg_brooder_final_CV2, train.dataset[seq(2, nrow(train.dataset), by = 5) ,]) - train.dataset$Tg_brooder[seq(2, nrow(train.dataset), by = 5)])^2 )/nrow(train.dataset) + MSE.LM.Tg_brooder_final
LM.Tg_brooder_final_CV3 = update(LM.Tg_brooder_final, data = train.dataset[ -seq(3, nrow(train.dataset), by = 5) ,] )
MSE.LM.Tg_brooder_final = sum( (predict(LM.Tg_brooder_final_CV3, train.dataset[seq(3, nrow(train.dataset), by = 5) ,]) - train.dataset$Tg_brooder[seq(3, nrow(train.dataset), by = 5)])^2 )/nrow(train.dataset) + MSE.LM.Tg_brooder_final
LM.Tg_brooder_final_CV4 = update(LM.Tg_brooder_final, data = train.dataset[ -seq(4, nrow(train.dataset), by = 5) ,] )
MSE.LM.Tg_brooder_final = sum( (predict(LM.Tg_brooder_final_CV4, train.dataset[seq(4, nrow(train.dataset), by = 5) ,]) - train.dataset$Tg_brooder[seq(4, nrow(train.dataset), by = 5)])^2 )/nrow(train.dataset) + MSE.LM.Tg_brooder_final
LM.Tg_brooder_final_CV5 = update(LM.Tg_brooder_final, data = train.dataset[ -seq(5, nrow(train.dataset), by = 5) ,] )
MSE.LM.Tg_brooder_final = sum( (predict(LM.Tg_brooder_final_CV5, train.dataset[seq(5, nrow(train.dataset), by = 5) ,]) - train.dataset$Tg_brooder[seq(5, nrow(train.dataset), by = 5)])^2 )/nrow(train.dataset) + MSE.LM.Tg_brooder_final
# retraining the model with all datapoints
LM.Tg_brooder_final_all = update(LM.Tg_brooder_final, data = all.dataset)
##### Tr
LM.Tr_1 = lm(Tr ~ Ta_brooder*Heat_source_power*Tg_brooder, data = train.dataset)
# summary(LM.Tr_1)
# anova(LM.Tr_1) # suggests to remove three-way interaction (p = 0.295)
LM.Tr_2 = lm(Tr ~ Ta_brooder*Heat_source_power + Ta_brooder*Tg_brooder + Heat_source_power*Tg_brooder, data = train.dataset)
# summary(LM.Tr_2)
# anova(LM.Tr_2) # suggests to remove all 2 two-way interactions for Tg_brooder (p >= 0.5696)
LM.Tr_3 = lm(Tr ~ Ta_brooder*Heat_source_power + Tg_brooder, data = train.dataset)
# summary(LM.Tr_3)
# anova(LM.Tr_3) # suggests to remove Tg_brooder altogether (p = 0.237)
LM.Tr_4 = lm(Tr ~ Ta_brooder*Heat_source_power, data = train.dataset)
# summary(LM.Tr_4)
# anova(LM.Tr_4) # suggests it's ok
LM.Tr_final = LM.Tr_4
# cross-validation
LM.Tr_final_CV1 = update(LM.Tr_final, data = train.dataset[ -seq(1, nrow(train.dataset), by = 5) ,] )
MSE.LM.Tr_final = sum( (predict(LM.Tr_final_CV1, train.dataset[seq(1, nrow(train.dataset), by = 5) ,]) - train.dataset$Tr[seq(1, nrow(train.dataset), by = 5)])^2 )/nrow(train.dataset)
anova(LM.Tr_4)
anova(LM.Tg_brooder_Ta_pen_2)
anova(LM.Ta_brooder_2, LM.Ta_brooder_1)
# getting the models names
models_all = getModelId(modelGrid, number_of_cores)
print(paste0("calculating errors in ", model_name))
initial_position = sum(c(1, number_of_models[0:(algo-1)] )) + sum(number_of_models)*(it - 1)
final_position = sum(number_of_models[0:algo] ) + sum(number_of_models)*(it - 1)
# Compensenting for cases where we might not find all models
if (final_position - initial_position > length(models_all)){
final_position = initial_position + length(models_all) - 1
}
output_calc_errors = calc_errors(models_all, train.h2o, test.h2o, mean.h2o, ys[it], ys_mean[it])
library(foreach)
# this function calculates the MSE, errors and percentages for training, test, and mean dataset.
# It assumes that the column pos_tt in train_dataset and test_dataset contain the true value
# and also that the column pos_mean contain the true value for mean_dataset
calc_errors <- function(models_all, train_dataset, test_dataset, mean_dataset, pos_tt, pos_mean){
number_of_cores = 8
# print("calc MSE")
# calculating predictions and performances
print("Calculating cross-validation MSE")
time_performance_CV_i = Sys.time()
performance_CV = mclapply(models_all, h2o.mse, xval = T, mc.cores = number_of_cores)
MSE_CV = unlist(performance_CV)
time_performance_CV = difftime(Sys.time(), time_performance_CV_i, units = "sec")
print("Calculating predictions and performance for training dataset")
time_prediction_training_i = Sys.time()
predictions_training = mclapply(models_all, h2o.predict, train_dataset, mc.cores = number_of_cores)
time_prediction_training_e = Sys.time()
performance_training = mclapply(models_all, h2o.mse, mc.cores = number_of_cores)
MSE_training = unlist(performance_training)
time_performance_training_e = Sys.time()
time_prediction_training = difftime(time_prediction_training_e, time_prediction_training_i, units = "sec")
time_performance_training = difftime(time_performance_training_e, time_prediction_training_e, units = "sec")
print("Calculating predictions and performance for testing dataset")
time_prediction_testing_i = Sys.time()
predictions_testing = mclapply(models_all, h2o.predict, test_dataset, mc.cores = number_of_cores)
time_prediction_testing_e = Sys.time()
performance_testing = mclapply(models_all, h2o.mse, valid = T, mc.cores = number_of_cores)
MSE_testing = unlist(performance_testing)
time_performance_testing_e = Sys.time()
time_prediction_testing = difftime(time_prediction_testing_e, time_prediction_testing_i, units = "sec")
time_performance_testing = difftime(time_performance_testing_e, time_prediction_training_e, units = "sec")
print("Calculating predictions and performance for mean dataset")
time_prediction_mean_i = Sys.time()
predictions_mean = mclapply(models_all, h2o.predict, mean_dataset, mc.cores = number_of_cores)
time_prediction_mean_e = Sys.time()
performance_mean = mclapply(models_all, h2o.performance, mean_dataset, mc.cores = number_of_cores)
time_performance_mean_e = Sys.time()
time_prediction_mean = difftime(time_prediction_mean_e, time_prediction_mean_i, units = "sec")
time_performance_mean = difftime(time_performance_mean_e, time_prediction_mean_e, units = "sec")
time_errors_i = Sys.time()
# calculating lengths
length_model = length(performance_training)
sqrt_nrow_train = sqrt(nrow(predictions_training[[1]]))
sqrt_nrow_test = sqrt(nrow(predictions_testing[[1]]))
sqrt_nrow_mean = sqrt(nrow(predictions_mean[[1]]))
train_dataset_values = as.data.frame(train_dataset[,pos_tt])
train_dataset_values = unlist(train_dataset_values)
test_dataset_values = as.data.frame(test_dataset[,pos_tt])
test_dataset_values = unlist(test_dataset_values)
mean_dataset_values = as.data.frame(mean_dataset[,pos_mean])
mean_dataset_values = unlist(mean_dataset_values)
print("Calculating errors for each model")
list_of_errors <- foreach (i = 1:length_model) %dopar% {
MSE_mean = performance_mean[[i]]@metrics$MSE
# predictions for errors
# getting values
predictions_training_i = as.data.frame(predictions_training[[i]])
predictions_training_i = unlist(predictions_training_i)
predictions_testing_i = as.data.frame(predictions_testing[[i]])
predictions_testing_i = unlist(predictions_testing_i)
predictions_mean_i = as.data.frame(predictions_mean[[i]])
predictions_mean_i = unlist(predictions_mean_i)
# training errors
error_training_mean = mean( abs(predictions_training_i - train_dataset_values ) )
error_training_SEM = sd(predictions_training_i - train_dataset_values )/sqrt_nrow_train
error_training_max = max( abs(predictions_training_i - train_dataset_values ) )
error_training_percent_mean = mean( abs(predictions_training_i - train_dataset_values )/train_dataset_values*100 )
error_training_percent_SEM = sd( (predictions_training_i - train_dataset_values)/train_dataset_values*100 )/sqrt_nrow_train
error_training_percent_max = max( abs(predictions_training_i - train_dataset_values )/train_dataset_values*100 )
# print("save prediction errors for testing")
error_testing_mean = mean( abs(predictions_testing_i - test_dataset_values ) )
error_testing_SEM = sd(predictions_testing_i - test_dataset_values )/sqrt_nrow_test
error_testing_max  = max( abs(predictions_testing_i - test_dataset_values ) )
error_testing_percent_mean = mean( abs(predictions_testing_i - test_dataset_values )/test_dataset_values*100 )
error_testing_percent_SEM = sd( (predictions_testing_i - test_dataset_values)/test_dataset_values*100 )/sqrt_nrow_test
error_testing_percent_max = max( abs(predictions_testing_i - test_dataset_values )/test_dataset_values*100 )
# print("save prediction errors for mean")
error_mean_mean = mean( abs(predictions_mean_i - mean_dataset_values ) )
error_mean_SEM = sd(predictions_mean_i - mean_dataset_values )/sqrt_nrow_mean
error_mean_max = max( abs(predictions_mean_i - mean_dataset_values ) )
error_mean_percent_mean = mean( abs(predictions_mean_i - mean_dataset_values )/mean_dataset_values*100 )
error_mean_percent_SEM = sd( (predictions_mean_i - mean_dataset_values)/mean_dataset_values*100 )/sqrt_nrow_mean
error_mean_percent_max = max( abs(predictions_mean_i - mean_dataset_values )/mean_dataset_values*100 )
return( c(MSE_mean,
error_training_mean, error_training_SEM, error_training_max,
error_training_percent_mean, error_training_percent_SEM, error_training_percent_max,
error_testing_mean, error_testing_SEM, error_testing_max,
error_testing_percent_mean, error_testing_percent_SEM, error_testing_percent_max,
error_mean_mean, error_mean_SEM, error_mean_max,
error_mean_percent_mean, error_mean_percent_SEM, error_mean_percent_max) )
}
time_errors = difftime(Sys.time(), time_errors_i, units = "sec")
matrix_errors = cbind( MSE_CV, MSE_training, MSE_testing, matrix(unlist(list_of_errors), ncol = 19, byrow = T) )
vector_times = c(time_performance_CV,
time_prediction_training, time_performance_training,
time_prediction_testing, time_performance_testing,
time_prediction_mean, time_performance_mean,time_errors)
print("Done calculating errors for each model")
return(list(matrix_errors, vector_times))
}
# wraper for getting the model ids for algorithms that use grid
# the ones that do not
getModelId <- function(models, n_cores){
# checking if it is was runned using the native algorithm
# or if it was run using grid
if (length(models) == 1){
# have run using grid and got n models
return( mclapply(as.character(models@model_ids), h2o.getModel, mc.cores = n_cores) )
} else if (.hasSlot(models[[1]], "algorithm")){
# have run using the native algorithm
return( mclapply(models, getModelId_algo, mc.cores = n_cores) )
} else{
# have run using grid and got output as a list
return( mclapply(models, getModelId_grid_1, mc.cores = n_cores) )
}
}
# return the model id for models runned using the native algorithm
getModelId_algo <- function(model){
return( h2o.getModel( model@model_id ) )
}
# return the model id for models runned using grid for only one model
getModelId_grid_1 <- function(model){
return( h2o.getModel( model@model_ids[[1]] ) )
}
# function to get grids running in parallel
parallel.general.grid = function(seed, ...){
model = h2o.grid(..., search_criteria = list(strategy = "RandomDiscrete", max_models = 1,
seed = seed))
return(model)
}
# function to do deep learning in parallel
parallel.deep.learning = function(seed, layers_DL, hidden_DL, epsilon_DL, rho_DL,
epochs_DL, mini_batch_size_DL, hidden_dropout_DL,
max_runtime_secs_DL,
X, Y, train, test, distribution_DL, fast_mode_DL,
activation_function_DL,
reproducible_DL, nfolds_DL) {
set.seed(seed)
# random search for hyperparameters
layers_temp <- sample(layers_DL, 1)
hidden_temp <- sample(hidden_DL, layers_temp, replace=TRUE)
epsilon_temp <- sample(epsilon_DL, 1)
rho_temp <- sample(rho_DL, 1)
epochs_temp <- sample(epochs_DL, 1)
mini_batch_size_temp <- sample(mini_batch_size_DL, 1)
hidden_dropout_temp <- sample(hidden_dropout_DL, layers_temp, replace=TRUE)
# running the model
model <- h2o.deeplearning(x=X, y=Y, training_frame = train, validation_frame = test,
distribution = distribution_DL, hidden = hidden_temp,
epsilon = epsilon_temp,
rho = rho_temp, epochs = epochs_temp,
mini_batch_size = mini_batch_size_temp,
hidden_dropout = hidden_dropout_temp,
max_runtime_secs = max_runtime_secs_DL,
fast_mode = fast_mode_DL,
activation = activation_function_DL,
reproducible = reproducible_DL, seed = seed,
keep_cross_validation_predictions = TRUE,
nfolds = nfolds_DL, fold_assignment = "Modulo")
return(model)
}
output_calc_errors = calc_errors(models_all, train.h2o, test.h2o, mean.h2o, ys[it], ys_mean[it])
registerDoMC(number_of_cores)
library(doMC)
registerDoMC(number_of_cores)
# this function calculates the MSE, errors and percentages for training, test, and mean dataset.
# It assumes that the column pos_tt in train_dataset and test_dataset contain the true value
# and also that the column pos_mean contain the true value for mean_dataset
calc_errors <- function(models_all, train_dataset, test_dataset, mean_dataset, pos_tt, pos_mean){
number_of_cores = 8
# print("calc MSE")
# calculating predictions and performances
print("Calculating cross-validation MSE")
time_performance_CV_i = Sys.time()
performance_CV = mclapply(models_all, h2o.mse, xval = T, mc.cores = number_of_cores)
MSE_CV = unlist(performance_CV)
time_performance_CV = difftime(Sys.time(), time_performance_CV_i, units = "sec")
print("Calculating predictions and performance for training dataset")
time_prediction_training_i = Sys.time()
predictions_training = mclapply(models_all, h2o.predict, train_dataset, mc.cores = number_of_cores)
time_prediction_training_e = Sys.time()
performance_training = mclapply(models_all, h2o.mse, mc.cores = number_of_cores)
MSE_training = unlist(performance_training)
time_performance_training_e = Sys.time()
time_prediction_training = difftime(time_prediction_training_e, time_prediction_training_i, units = "sec")
time_performance_training = difftime(time_performance_training_e, time_prediction_training_e, units = "sec")
print("Calculating predictions and performance for testing dataset")
time_prediction_testing_i = Sys.time()
predictions_testing = mclapply(models_all, h2o.predict, test_dataset, mc.cores = number_of_cores)
time_prediction_testing_e = Sys.time()
performance_testing = mclapply(models_all, h2o.mse, valid = T, mc.cores = number_of_cores)
MSE_testing = unlist(performance_testing)
time_performance_testing_e = Sys.time()
time_prediction_testing = difftime(time_prediction_testing_e, time_prediction_testing_i, units = "sec")
time_performance_testing = difftime(time_performance_testing_e, time_prediction_training_e, units = "sec")
print("Calculating predictions and performance for mean dataset")
time_prediction_mean_i = Sys.time()
predictions_mean = mclapply(models_all, h2o.predict, mean_dataset, mc.cores = number_of_cores)
time_prediction_mean_e = Sys.time()
performance_mean = mclapply(models_all, h2o.performance, mean_dataset, mc.cores = number_of_cores)
time_performance_mean_e = Sys.time()
time_prediction_mean = difftime(time_prediction_mean_e, time_prediction_mean_i, units = "sec")
time_performance_mean = difftime(time_performance_mean_e, time_prediction_mean_e, units = "sec")
time_errors_i = Sys.time()
# calculating lengths
length_model = length(performance_training)
sqrt_nrow_train = sqrt(nrow(predictions_training[[1]]))
sqrt_nrow_test = sqrt(nrow(predictions_testing[[1]]))
sqrt_nrow_mean = sqrt(nrow(predictions_mean[[1]]))
train_dataset_values = as.data.frame(train_dataset[,pos_tt])
train_dataset_values = unlist(train_dataset_values)
test_dataset_values = as.data.frame(test_dataset[,pos_tt])
test_dataset_values = unlist(test_dataset_values)
mean_dataset_values = as.data.frame(mean_dataset[,pos_mean])
mean_dataset_values = unlist(mean_dataset_values)
print("Calculating errors for each model")
list_of_errors <- foreach (i = 1:length_model) %dopar% {
MSE_mean = performance_mean[[i]]@metrics$MSE
# predictions for errors
# getting values
predictions_training_i = as.data.frame(predictions_training[[i]])
predictions_training_i = unlist(predictions_training_i)
predictions_testing_i = as.data.frame(predictions_testing[[i]])
predictions_testing_i = unlist(predictions_testing_i)
predictions_mean_i = as.data.frame(predictions_mean[[i]])
predictions_mean_i = unlist(predictions_mean_i)
# training errors
error_training_mean = mean( abs(predictions_training_i - train_dataset_values ) )
error_training_SEM = sd(predictions_training_i - train_dataset_values )/sqrt_nrow_train
error_training_max = max( abs(predictions_training_i - train_dataset_values ) )
error_training_percent_mean = mean( abs(predictions_training_i - train_dataset_values )/train_dataset_values*100 )
error_training_percent_SEM = sd( (predictions_training_i - train_dataset_values)/train_dataset_values*100 )/sqrt_nrow_train
error_training_percent_max = max( abs(predictions_training_i - train_dataset_values )/train_dataset_values*100 )
# print("save prediction errors for testing")
error_testing_mean = mean( abs(predictions_testing_i - test_dataset_values ) )
error_testing_SEM = sd(predictions_testing_i - test_dataset_values )/sqrt_nrow_test
error_testing_max  = max( abs(predictions_testing_i - test_dataset_values ) )
error_testing_percent_mean = mean( abs(predictions_testing_i - test_dataset_values )/test_dataset_values*100 )
error_testing_percent_SEM = sd( (predictions_testing_i - test_dataset_values)/test_dataset_values*100 )/sqrt_nrow_test
error_testing_percent_max = max( abs(predictions_testing_i - test_dataset_values )/test_dataset_values*100 )
# print("save prediction errors for mean")
error_mean_mean = mean( abs(predictions_mean_i - mean_dataset_values ) )
error_mean_SEM = sd(predictions_mean_i - mean_dataset_values )/sqrt_nrow_mean
error_mean_max = max( abs(predictions_mean_i - mean_dataset_values ) )
error_mean_percent_mean = mean( abs(predictions_mean_i - mean_dataset_values )/mean_dataset_values*100 )
error_mean_percent_SEM = sd( (predictions_mean_i - mean_dataset_values)/mean_dataset_values*100 )/sqrt_nrow_mean
error_mean_percent_max = max( abs(predictions_mean_i - mean_dataset_values )/mean_dataset_values*100 )
return( c(MSE_mean,
error_training_mean, error_training_SEM, error_training_max,
error_training_percent_mean, error_training_percent_SEM, error_training_percent_max,
error_testing_mean, error_testing_SEM, error_testing_max,
error_testing_percent_mean, error_testing_percent_SEM, error_testing_percent_max,
error_mean_mean, error_mean_SEM, error_mean_max,
error_mean_percent_mean, error_mean_percent_SEM, error_mean_percent_max) )
}
time_errors = difftime(Sys.time(), time_errors_i, units = "sec")
matrix_errors = cbind( MSE_CV, MSE_training, MSE_testing, matrix(unlist(list_of_errors), ncol = 19, byrow = T) )
vector_times = c(time_performance_CV,
time_prediction_training, time_performance_training,
time_prediction_testing, time_performance_testing,
time_prediction_mean, time_performance_mean,time_errors)
print("Done calculating errors for each model")
return(list(matrix_errors, vector_times))
}
# wraper for getting the model ids for algorithms that use grid
# the ones that do not
getModelId <- function(models, n_cores){
# checking if it is was runned using the native algorithm
# or if it was run using grid
if (length(models) == 1){
# have run using grid and got n models
return( mclapply(as.character(models@model_ids), h2o.getModel, mc.cores = n_cores) )
} else if (.hasSlot(models[[1]], "algorithm")){
# have run using the native algorithm
return( mclapply(models, getModelId_algo, mc.cores = n_cores) )
} else{
# have run using grid and got output as a list
return( mclapply(models, getModelId_grid_1, mc.cores = n_cores) )
}
}
# return the model id for models runned using the native algorithm
getModelId_algo <- function(model){
return( h2o.getModel( model@model_id ) )
}
# return the model id for models runned using grid for only one model
getModelId_grid_1 <- function(model){
return( h2o.getModel( model@model_ids[[1]] ) )
}
# function to get grids running in parallel
parallel.general.grid = function(seed, ...){
model = h2o.grid(..., search_criteria = list(strategy = "RandomDiscrete", max_models = 1,
seed = seed))
return(model)
}
# function to do deep learning in parallel
parallel.deep.learning = function(seed, layers_DL, hidden_DL, epsilon_DL, rho_DL,
epochs_DL, mini_batch_size_DL, hidden_dropout_DL,
max_runtime_secs_DL,
X, Y, train, test, distribution_DL, fast_mode_DL,
activation_function_DL,
reproducible_DL, nfolds_DL) {
set.seed(seed)
# random search for hyperparameters
layers_temp <- sample(layers_DL, 1)
hidden_temp <- sample(hidden_DL, layers_temp, replace=TRUE)
epsilon_temp <- sample(epsilon_DL, 1)
rho_temp <- sample(rho_DL, 1)
epochs_temp <- sample(epochs_DL, 1)
mini_batch_size_temp <- sample(mini_batch_size_DL, 1)
hidden_dropout_temp <- sample(hidden_dropout_DL, layers_temp, replace=TRUE)
# running the model
model <- h2o.deeplearning(x=X, y=Y, training_frame = train, validation_frame = test,
distribution = distribution_DL, hidden = hidden_temp,
epsilon = epsilon_temp,
rho = rho_temp, epochs = epochs_temp,
mini_batch_size = mini_batch_size_temp,
hidden_dropout = hidden_dropout_temp,
max_runtime_secs = max_runtime_secs_DL,
fast_mode = fast_mode_DL,
activation = activation_function_DL,
reproducible = reproducible_DL, seed = seed,
keep_cross_validation_predictions = TRUE,
nfolds = nfolds_DL, fold_assignment = "Modulo")
return(model)
}
output_calc_errors = calc_errors(models_all, train.h2o, test.h2o, mean.h2o, ys[it], ys_mean[it])
