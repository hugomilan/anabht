print("Calculating predictions and performance for testing dataset")
time_prediction_testing_i = Sys.time()
predictions_testing = mclapply(models_all, h2o.predict, test_dataset, mc.cores = number_of_cores)
time_prediction_testing_e = Sys.time()
performance_testing = mclapply(models_all, h2o.mse, valid = T, mc.cores = number_of_cores)
MSE_testing = unlist(performance_testing)
time_performance_testing_e = Sys.time()
time_prediction_testing = difftime(time_prediction_testing_e, time_prediction_testing_i, units = "sec")
time_performance_testing = difftime(time_performance_testing_e, time_prediction_training_e, units = "sec")
print("Calculating predictions and performance for mean dataset")
time_prediction_mean_i = Sys.time()
predictions_mean = mclapply(models_all, h2o.predict, mean_dataset, mc.cores = number_of_cores)
time_prediction_mean_e = Sys.time()
performance_mean = mclapply(models_all, h2o.performance, mean_dataset, mc.cores = number_of_cores)
time_performance_mean_e = Sys.time()
time_prediction_mean = difftime(time_prediction_mean_e, time_prediction_mean_i, units = "sec")
time_performance_mean = difftime(time_performance_mean_e, time_prediction_mean_e, units = "sec")
time_errors_i = Sys.time()
# calculating lengths
length_model = length(performance_training)
sqrt_nrow_train = sqrt(nrow(predictions_training[[1]]))
sqrt_nrow_test = sqrt(nrow(predictions_testing[[1]]))
sqrt_nrow_mean = sqrt(nrow(predictions_mean[[1]]))
train_dataset_values = as.data.frame(train_dataset[,pos_tt])
train_dataset_values = unlist(train_dataset_values)
test_dataset_values = as.data.frame(test_dataset[,pos_tt])
test_dataset_values = unlist(test_dataset_values)
mean_dataset_values = as.data.frame(mean_dataset[,pos_mean])
mean_dataset_values = unlist(mean_dataset_values)
print("Calculating errors for each model")
list_of_errors <- foreach (i = 1:length_model) %dopar% {
MSE_mean = performance_mean[[i]]@metrics$MSE
# predictions for errors
# getting values
predictions_training_i = as.data.frame(predictions_training[[i]])
predictions_training_i = unlist(predictions_training_i)
predictions_testing_i = as.data.frame(predictions_testing[[i]])
predictions_testing_i = unlist(predictions_testing_i)
predictions_mean_i = as.data.frame(predictions_mean[[i]])
predictions_mean_i = unlist(predictions_mean_i)
# training errors
error_training_mean = mean( abs(predictions_training_i - train_dataset_values ) )
error_training_SEM = sd(predictions_training_i - train_dataset_values )/sqrt_nrow_train
error_training_max = max( abs(predictions_training_i - train_dataset_values ) )
error_training_percent_mean = mean( abs(predictions_training_i - train_dataset_values )/train_dataset_values*100 )
error_training_percent_SEM = sd( (predictions_training_i - train_dataset_values)/train_dataset_values*100 )/sqrt_nrow_train
error_training_percent_max = max( abs(predictions_training_i - train_dataset_values )/train_dataset_values*100 )
# print("save prediction errors for testing")
error_testing_mean = mean( abs(predictions_testing_i - test_dataset_values ) )
error_testing_SEM = sd(predictions_testing_i - test_dataset_values )/sqrt_nrow_test
error_testing_max  = max( abs(predictions_testing_i - test_dataset_values ) )
error_testing_percent_mean = mean( abs(predictions_testing_i - test_dataset_values )/test_dataset_values*100 )
error_testing_percent_SEM = sd( (predictions_testing_i - test_dataset_values)/test_dataset_values*100 )/sqrt_nrow_test
error_testing_percent_max = max( abs(predictions_testing_i - test_dataset_values )/test_dataset_values*100 )
# print("save prediction errors for mean")
error_mean_mean = mean( abs(predictions_mean_i - mean_dataset_values ) )
error_mean_SEM = sd(predictions_mean_i - mean_dataset_values )/sqrt_nrow_mean
error_mean_max = max( abs(predictions_mean_i - mean_dataset_values ) )
error_mean_percent_mean = mean( abs(predictions_mean_i - mean_dataset_values )/mean_dataset_values*100 )
error_mean_percent_SEM = sd( (predictions_mean_i - mean_dataset_values)/mean_dataset_values*100 )/sqrt_nrow_mean
error_mean_percent_max = max( abs(predictions_mean_i - mean_dataset_values )/mean_dataset_values*100 )
return( c(MSE_mean,
error_training_mean, error_training_SEM, error_training_max,
error_training_percent_mean, error_training_percent_SEM, error_training_percent_max,
error_testing_mean, error_testing_SEM, error_testing_max,
error_testing_percent_mean, error_testing_percent_SEM, error_testing_percent_max,
error_mean_mean, error_mean_SEM, error_mean_max,
error_mean_percent_mean, error_mean_percent_SEM, error_mean_percent_max) )
}
time_errors = difftime(Sys.time(), time_errors_i, units = "sec")
matrix_errors = cbind( MSE_CV, MSE_training, MSE_testing, matrix(unlist(list_of_errors), ncol = 19, byrow = T) )
vector_times = c(time_performance_CV,
time_prediction_training, time_performance_training,
time_prediction_testing, time_performance_testing,
time_prediction_mean, time_performance_mean,time_errors)
print("Done calculating errors for each model")
return(list(matrix_errors, vector_times))
}
# wraper for getting the model ids for algorithms that use grid
# the ones that do not
getModelId <- function(models, n_cores){
# checking if it is was runned using the native algorithm
# or if it was run using grid
if (length(models) == 1){
# have run using grid and got n models
return( mclapply(as.character(models@model_ids), h2o.getModel, mc.cores = n_cores) )
} else if (.hasSlot(models[[1]], "algorithm")){
# have run using the native algorithm
return( mclapply(models, getModelId_algo, mc.cores = n_cores) )
} else{
# have run using grid and got output as a list
return( mclapply(models, getModelId_grid_1, mc.cores = n_cores) )
}
}
# return the model id for models runned using the native algorithm
getModelId_algo <- function(model){
return( h2o.getModel( model@model_id ) )
}
# return the model id for models runned using grid for only one model
getModelId_grid_1 <- function(model){
return( h2o.getModel( model@model_ids[[1]] ) )
}
# function to get grids running in parallel
parallel.general.grid = function(seed, ...){
model = h2o.grid(..., search_criteria = list(strategy = "RandomDiscrete", max_models = 1,
seed = seed))
return(model)
}
# function to do deep learning in parallel
parallel.deep.learning = function(seed, layers_DL, hidden_DL, epsilon_DL, rho_DL,
epochs_DL, mini_batch_size_DL, hidden_dropout_DL,
max_runtime_secs_DL,
X, Y, train, test, distribution_DL, fast_mode_DL,
activation_function_DL,
reproducible_DL, nfolds_DL) {
set.seed(seed)
# random search for hyperparameters
layers_temp <- sample(layers_DL, 1)
hidden_temp <- sample(hidden_DL, layers_temp, replace=TRUE)
epsilon_temp <- sample(epsilon_DL, 1)
rho_temp <- sample(rho_DL, 1)
epochs_temp <- sample(epochs_DL, 1)
mini_batch_size_temp <- sample(mini_batch_size_DL, 1)
hidden_dropout_temp <- sample(hidden_dropout_DL, layers_temp, replace=TRUE)
# running the model
model <- h2o.deeplearning(x=X, y=Y, training_frame = train, validation_frame = test,
distribution = distribution_DL, hidden = hidden_temp,
epsilon = epsilon_temp,
rho = rho_temp, epochs = epochs_temp,
mini_batch_size = mini_batch_size_temp,
hidden_dropout = hidden_dropout_temp,
max_runtime_secs = max_runtime_secs_DL,
fast_mode = fast_mode_DL,
activation = activation_function_DL,
reproducible = reproducible_DL, seed = seed,
keep_cross_validation_predictions = TRUE,
nfolds = nfolds_DL, fold_assignment = "Modulo")
return(model)
}
# independent and dependent variables
X.list = list()
X.list[[1]] = c(5, 13) # c("Heat_source_power", "Ta_pen") # heat_source_power, Ta_pen
X.list[[2]] = c(5, 10) # c("Heat_source_power", "Ta_brooder") # heat_source_power, Ta_brooder
X.list[[3]] = c(5, 10, 12) # c("Heat_source_power", "Ta_brooder", "Tg_brooder") # heat_source_power, Ta_brooder, Tg_brooder
X.list[[4]] = c(5, 13) # c("Heat_source_power", "Ta_pen") # heat_source_power, Ta_pen
X.list[[5]] = c(5, 13) # c("Heat_source_power", "Ta_pen") # heat_source_power, Ta_pen
X.list[[6]] = c(5, 10) # c("Heat_source_power", "Ta_brooder") # heat_source_power, Ta_brooder
interaction_list = list()
interaction_list[[1]] = c("Heat_source_power", "Ta_pen")
interaction_list[[2]] = c("Heat_source_power", "Ta_brooder")
interaction_list[[3]] = c("Heat_source_power", "Ta_brooder", "Tg_brooder")
interaction_list[[4]] = c("Heat_source_power", "Ta_pen")
interaction_list[[5]] = c("Heat_source_power", "Ta_pen")
interaction_list[[6]] = c("Heat_source_power", "Ta_brooder")
variableName = c("Ta_brooder", "Tg_brooder", "Tr", "Tg_brooder_Ta_pen", "Tr_Ta_pen", "Tr_Ta_brooder")
ys = c("Ta_brooder", "Tg_brooder", "Tr", "Tg_brooder", "Tr", "Tr") # c(10, 12, 8) # Ta_brooder, Tg_brooder, Tr, Tg_brooder, Tr, Tr
ys_mean = c(5, 7, 3, 7, 3, 3) # Ta_brooder, Tg_brooder, Tr, Tg_brooder, Tr, Tr
# number of models
# GLM parameters
alpha_GLM <- seq(0, 1, .000001)
lambda_GLM <- 10^(seq(-10, 0, 0.00001))
number_of_GLM_models = 1000
seeds_GLM = 1:number_of_GLM_models # the length of the seeds_GLM defines the number of models
family_GLM = "gaussian" # Qualitatively appears to be normally distributed data
max_iterations_GLM = 200
nfolds_GLM = 5
lambda_search_GLM = FALSE # lambda_search_GLM = TRUE was faster but yielded worser models
nlambdas_GLM = 1000 # number of lambdas to search if lambda_search_GLM = TRUE
use_grid_all_GLM = TRUE # should all models be trained in the grid or each model individually?
max_runtime_secs_GLM = 0*( 1 + (number_of_GLM_models - 1)*use_grid_all_GLM ) # the time each model will at most run in seconds. If using grid_all, we multiply the time by the number of models
train_GLM = TRUE
# RF parameters
number_of_RF_models = 1000
seeds_RF = 1:number_of_RF_models # the length of the seeds_RF defines the number of models
min_rows_RF <- 1:30
ntrees_RF <- 10:250
max_depth_RF <- 1:100
distribution_RF <- "gaussian"
#mtries_RF <- 1:length(X)
nfolds_RF = 5
max_runtime_secs_RF = 0 # the time each model will at most run in seconds
train_RF = TRUE
# GBM parameters
number_of_GBM_models = 1000
seeds_GBM = 1:number_of_GBM_models # the length of the seeds_GBM defines the number of models
min_rows_GBM <- 1:20
ntrees_GBM <- 1:100
max_depth_GBM <- 1:100
distribution_GBM <- "gaussian"
learn_rate_GBM <- seq(0.001, 1, 0.001)
learn_rate_annealing_GBM <- seq(0.8, 1, .001)
nfolds_GBM = 5
max_runtime_secs_GBM = 0 # the time each model will at most run in seconds
train_GBM = TRUE
# DL paramenters
number_of_DL_models = 2000
seeds_DL = 1:number_of_DL_models # the length of the seeds_GBM defines the number of models
layers_DL = 1:10
hidden_DL = 1:250
epsilon_DL <- 10^seq(-10, -6, .000001)
rho_DL <- seq(.75, .999, .000001)
distribution_DL <- "gaussian"
epochs_DL <- 1:10000
mini_batch_size_DL <- 1:nrow(train.dataset)
fast_mode_DL <- FALSE
reproducible_DL <- TRUE # running with FALSE didn't converge (didn't want to wait to see if it would)
hidden_dropout_DL <- seq(0, .33, .01)
# Cannot grid mini-batch-size
activation_function_DL <- "RectifierWithDropout"
nfolds_DL = 5
max_runtime_secs_DL = 0 # the time each model will at most run in seconds
train_DL = TRUE
# total number of models
number_of_models = c(number_of_GLM_models, number_of_RF_models,
number_of_GBM_models, number_of_DL_models) # GLM, RF, GBM, DL
total_number_of_models = sum(number_of_models)
# creating things for the loop
# dataset for all models
nVariblesToPredict = length(X.list)
MSE_all_dataset = data.frame(algorithm = rep( c( rep("GLM", number_of_GLM_models), rep("RF", number_of_RF_models), rep("GBM", number_of_GBM_models), rep("DL", number_of_DL_models) ), nVariblesToPredict),
model = rep(c(1:number_of_GLM_models, 1:number_of_RF_models, 1:number_of_GBM_models, 1:number_of_DL_models), nVariblesToPredict),
MSE_CV = rep(Inf, total_number_of_models*nVariblesToPredict),
MSE_training = rep(Inf, total_number_of_models*nVariblesToPredict),
MSE_testing = rep(Inf, total_number_of_models*nVariblesToPredict),
MSE_mean = rep(Inf, total_number_of_models*nVariblesToPredict),
error_training_mean = rep(Inf, total_number_of_models*nVariblesToPredict),
error_training_SEM = rep(Inf, total_number_of_models*nVariblesToPredict),
error_training_max = rep(Inf, total_number_of_models*nVariblesToPredict),
error_training_percent_mean = rep(Inf, total_number_of_models*nVariblesToPredict),
error_training_percent_SEM = rep(Inf, total_number_of_models*nVariblesToPredict),
error_training_percent_max = rep(Inf, total_number_of_models*nVariblesToPredict),
error_testing_mean = rep(Inf, total_number_of_models*nVariblesToPredict),
error_testing_SEM = rep(Inf, total_number_of_models*nVariblesToPredict),
error_testing_max = rep(Inf, total_number_of_models*nVariblesToPredict),
error_testing_percent_mean = rep(Inf, total_number_of_models*nVariblesToPredict),
error_testing_percent_SEM = rep(Inf, total_number_of_models*nVariblesToPredict),
error_testing_percent_max = rep(Inf, total_number_of_models*nVariblesToPredict),
error_mean_mean = rep(Inf, total_number_of_models*nVariblesToPredict),
error_mean_SEM = rep(Inf, total_number_of_models*nVariblesToPredict),
error_mean_max = rep(Inf, total_number_of_models*nVariblesToPredict),
error_mean_percent_mean = rep(Inf, total_number_of_models*nVariblesToPredict),
error_mean_percent_SEM = rep(Inf, total_number_of_models*nVariblesToPredict),
error_mean_percent_max = rep(Inf, total_number_of_models*nVariblesToPredict))
# total_number_of_models x nVariblesToPredict predicting variables
# only for the best models + times and other stuffs. Best model is defined as the onw that minimizes MSE_CV
MSE_best_dataset = data.frame(algorithm = rep( c("GLM", "RF", "GBM", "DL"), nVariblesToPredict),
model = rep(0, 4*nVariblesToPredict),
MSE_CV = rep(Inf, 4*nVariblesToPredict),
MSE_training = rep(Inf, 4*nVariblesToPredict),
MSE_testing = rep(Inf, 4*nVariblesToPredict),
MSE_mean = rep(Inf, 4*nVariblesToPredict),
error_training_mean = rep(Inf, 4*nVariblesToPredict),
error_training_SEM = rep(Inf, 4*nVariblesToPredict),
error_training_max = rep(Inf, 4*nVariblesToPredict),
error_training_percent_mean = rep(Inf, 4*nVariblesToPredict),
error_training_percent_SEM = rep(Inf, 4*nVariblesToPredict),
error_training_percent_max = rep(Inf, 4*nVariblesToPredict),
error_testing_mean = rep(Inf, 4*nVariblesToPredict),
error_testing_SEM = rep(Inf, 4*nVariblesToPredict),
error_testing_max = rep(Inf, 4*nVariblesToPredict),
error_testing_percent_mean = rep(Inf, 4*nVariblesToPredict),
error_testing_percent_SEM = rep(Inf, 4*nVariblesToPredict),
error_testing_percent_max = rep(Inf, 4*nVariblesToPredict),
error_mean_mean = rep(Inf, 4*nVariblesToPredict),
error_mean_SEM = rep(Inf, 4*nVariblesToPredict),
error_mean_max = rep(Inf, 4*nVariblesToPredict),
error_mean_percent_mean = rep(Inf, 4*nVariblesToPredict),
error_mean_percent_SEM = rep(Inf, 4*nVariblesToPredict),
error_mean_percent_max = rep(Inf, 4*nVariblesToPredict),
algorithm_run_time = rep(Inf, 4*nVariblesToPredict),
time_performance_CV = rep(Inf, 4*nVariblesToPredict),
time_prediction_training = rep(Inf, 4*nVariblesToPredict),
time_performance_training = rep(Inf, 4*nVariblesToPredict),
time_prediction_testing = rep(Inf, 4*nVariblesToPredict),
time_performance_testing = rep(Inf, 4*nVariblesToPredict),
time_prediction_mean = rep(Inf, 4*nVariblesToPredict),
time_performance_mean = rep(Inf, 4*nVariblesToPredict),
time_errors = rep(Inf, 4*nVariblesToPredict),
n_models = rep(0, 4*nVariblesToPredict),
n_training = rep(nrow(train.dataset), 4*nVariblesToPredict),
n_testing = rep(nrow(test.dataset), 4*nVariblesToPredict),
n_mean = rep(nrow(mean.dataset), 4*nVariblesToPredict))
# GLM Tg_brooder_Ta_pen
GLM_Tg_brooder_Ta_pen = h2o.loadModel("Best ML models/GLM_Tg_brooder_Ta_pen")
GLM_Tg_brooder_Ta_pen_all = h2o.glm(x = X.list[[4]], y = ys[4], training_frame = all.h2o,
family = family_GLM, alpha = alpha_GLM[1], lambda = lambda_GLM[1],
lambda_search = lambda_search_GLM, max_iterations = max_iterations_GLM,
max_runtime_secs = max_runtime_secs_GLM, nfolds = nfolds_GLM,
fold_assignment = "Modulo", nlambdas = 1,
remove_collinear_columns=TRUE,
interactions = interaction_list[[1]])
h2o.saveModel(GLM_Tg_brooder_Ta_pen_all, "GLM/Tg_brooder_Ta_pen/all")
h2o.saveMojo(GLM_Tg_brooder_Ta_pen_all, "Best ML models/GLM_Tg_brooder_Ta_pen_all.MOJO")
# GLM Tr_Ta_pen
GLM_Tr_Ta_pen = h2o.loadModel("Best ML models/GLM_Tr_Ta_pen")
GLM_Tr_Ta_pen_all = h2o.glm(x = X.list[[5]], y = ys[5], training_frame = all.h2o,
family = family_GLM, alpha = alpha_GLM[1], lambda = lambda_GLM[1],
lambda_search = lambda_search_GLM, max_iterations = max_iterations_GLM,
max_runtime_secs = max_runtime_secs_GLM, nfolds = nfolds_GLM,
fold_assignment = "Modulo", nlambdas = 1,
remove_collinear_columns=TRUE,
interactions = interaction_list[[5]])
h2o.saveModel(GLM_Tr_Ta_pen_all, "GLM/Tr_Ta_pen/all")
h2o.saveMojo(GLM_Tr_Ta_pen_all, "Best ML models/GLM_Tr_Ta_pen_all.MOJO")
# GLM Tr_Ta_brooder
GLM_Tr_Ta_brooder = h2o.loadModel("Best ML models/GLM_Tr_Ta_brooder")
GLM_Tr_Ta_brooder_all = h2o.glm(x = X.list[[6]], y = ys[6], training_frame = all.h2o,
family = family_GLM, alpha = alpha_GLM[1], lambda = lambda_GLM[1],
lambda_search = lambda_search_GLM, max_iterations = max_iterations_GLM,
max_runtime_secs = max_runtime_secs_GLM, nfolds = nfolds_GLM,
fold_assignment = "Modulo", nlambdas = 1,
remove_collinear_columns=TRUE,
interactions = interaction_list[[6]])
h2o.saveModel(GLM_Tr_Ta_brooder_all, "GLM/Tr_Ta_brooder/all")
h2o.saveMojo(GLM_Tr_Ta_brooder_all, "Best ML models/GLM_Tr_Ta_brooder_all.MOJO")
# RF Tg_brooder
RF_Tg_brooder_Ta_pen = h2o.loadModel("Best ML models/RF_Tg_brooder_Ta_pen")
RF_Tg_brooder_Ta_pen_all.grid = parallel.general.grid(seed = 402, algorithm = "randomForest",
x=X.list[[4]], y=ys[4], training_frame = all.h2o,
distribution = distribution_RF,
keep_cross_validation_predictions = TRUE,
max_runtime_secs = max_runtime_secs_RF,
hyper_params = list(min_rows = min_rows_RF,
max_depth = max_depth_RF,
mtries = 1:length(X.list[[4]]),
ntrees = ntrees_RF),
nfolds = nfolds_RF, fold_assignment = "Modulo")
RF_Tg_brooder_Ta_pen_all = h2o.getModel(RF_Tg_brooder_Ta_pen_all.grid@model_ids[[1]])
h2o.saveModel(RF_Tg_brooder_Ta_pen_all, "RF/Tg_brooder_Ta_pen/all")
h2o.saveMojo(RF_Tg_brooder_Ta_pen_all, "Best ML models/RF_Tg_brooder_Ta_pen_all.MOJO")
# RF Tr_Ta_pen
RF_Tr_Ta_pen = h2o.loadModel("Best ML models/RF_Tr_Ta_pen")
RF_Tr_Ta_pen_all.grid = parallel.general.grid(seed = 48, algorithm = "randomForest",
x=X.list[[5]], y=ys[5], training_frame = all.h2o,
distribution = distribution_RF,
keep_cross_validation_predictions = TRUE,
max_runtime_secs = max_runtime_secs_RF,
hyper_params = list(min_rows = min_rows_RF,
max_depth = max_depth_RF,
mtries = 1:length(X.list[[5]]),
ntrees = ntrees_RF),
nfolds = nfolds_RF, fold_assignment = "Modulo")
RF_Tr_Ta_pen_all = h2o.getModel(RF_Tr_Ta_pen_all.grid@model_ids[[1]])
h2o.saveModel(RF_Tr_Ta_pen_all, "RF/Tr_Ta_pen/all")
h2o.saveMojo(RF_Tr_Ta_pen_all, "Best ML models/RF_Tr_Ta_pen_all.MOJO")
# RF Tr_Ta_brooder
RF_Tr_Ta_brooder = h2o.loadModel("Best ML models/RF_Tr_Ta_brooder")
RF_Tr_Ta_brooder_all.grid = parallel.general.grid(seed = 989, algorithm = "randomForest",
x=X.list[[6]], y=ys[6], training_frame = all.h2o,
distribution = distribution_RF,
keep_cross_validation_predictions = TRUE,
max_runtime_secs = max_runtime_secs_RF,
hyper_params = list(min_rows = min_rows_RF,
max_depth = max_depth_RF,
mtries = 1:length(X.list[[6]]),
ntrees = ntrees_RF),
nfolds = nfolds_RF, fold_assignment = "Modulo")
RF_Tr_Ta_brooder_all = h2o.getModel(RF_Tr_Ta_brooder_all.grid@model_ids[[1]])
h2o.saveModel(RF_Tr_Ta_brooder_all, "RF/Tr_Ta_brooder/all")
h2o.saveMojo(RF_Tr_Ta_brooder_all, "Best ML models/RF_Tr_Ta_brooder_all.MOJO")
# GBM Tg_brooder_Ta_pen
GBM_Tg_brooder_Ta_pen = h2o.loadModel("Best ML models/GBM_Tg_brooder_Ta_pen")
GBM_Tg_brooder_Ta_pen_all.grid = parallel.general.grid(seed = 877, algorithm = "gbm",
x=X.list[[4]], y=ys[4], training_frame = all.h2o,
distribution = distribution_GBM,
keep_cross_validation_predictions = TRUE,
max_runtime_secs = max_runtime_secs_GBM,
hyper_params = list(min_rows = min_rows_GBM,
max_depth = max_depth_GBM,
learn_rate = learn_rate_GBM,
ntrees = ntrees_GBM,
learn_rate_annealing = learn_rate_annealing_GBM),
nfolds = nfolds_GBM, fold_assignment = "Modulo")
GBM_Tg_brooder_Ta_pen_all = h2o.getModel(GBM_Tg_brooder_Ta_pen_all.grid@model_ids[[1]])
h2o.saveModel(GBM_Tg_brooder_Ta_pen_all, "GBM/Tg_brooder_Ta_pen/all")
h2o.saveMojo(GBM_Tg_brooder_Ta_pen_all, "Best ML models/GBM_Tg_brooder_Ta_pen_all.MOJO")
# GBM Tr_Ta_pen
GBM_Tr_Ta_pen = h2o.loadModel("Best ML models/GBM_Tr_Ta_pen")
GBM_Tr_Ta_pen_all.grid = parallel.general.grid(seed = 807, algorithm = "gbm",
x=X.list[[5]], y=ys[5], training_frame = all.h2o,
distribution = distribution_GBM,
keep_cross_validation_predictions = TRUE,
max_runtime_secs = max_runtime_secs_GBM,
hyper_params = list(min_rows = min_rows_GBM,
max_depth = max_depth_GBM,
learn_rate = learn_rate_GBM,
ntrees = ntrees_GBM,
learn_rate_annealing = learn_rate_annealing_GBM),
nfolds = nfolds_GBM, fold_assignment = "Modulo")
GBM_Tr_Ta_pen_all = h2o.getModel(GBM_Tr_Ta_pen_all.grid@model_ids[[1]])
h2o.saveModel(GBM_Tr_Ta_pen_all, "GBM/Tr_Ta_pen/all")
h2o.saveMojo(GBM_Tr_Ta_pen_all, "Best ML models/GBM_Tr_Ta_pen_all.MOJO")
# GBM Tr_Ta_brooder
GBM_Tr_Ta_brooder = h2o.loadModel("Best ML models/GBM_Tr_Ta_brooder")
GBM_Tr_Ta_brooder_all.grid = parallel.general.grid(seed = 498, algorithm = "gbm",
x=X.list[[6]], y=ys[6], training_frame = all.h2o,
distribution = distribution_GBM,
keep_cross_validation_predictions = TRUE,
max_runtime_secs = max_runtime_secs_GBM,
hyper_params = list(min_rows = min_rows_GBM,
max_depth = max_depth_GBM,
learn_rate = learn_rate_GBM,
ntrees = ntrees_GBM,
learn_rate_annealing = learn_rate_annealing_GBM),
nfolds = nfolds_GBM, fold_assignment = "Modulo")
GBM_Tr_Ta_brooder_all = h2o.getModel(GBM_Tr_Ta_brooder_all.grid@model_ids[[1]])
h2o.saveModel(GBM_Tr_Ta_brooder_all, "GBM/Tr_Ta_brooder/all")
h2o.saveMojo(GBM_Tr_Ta_brooder_all, "Best ML models/GBM_Tr_Ta_brooder_all.MOJO")
# DL Tg_brooder_Ta_pen
DL_Tg_brooder_Ta_pen = h2o.loadModel("Best ML models/DL_Tg_brooder_Ta_pen")
DL_Tg_brooder_Ta_pen_all = parallel.deep.learning(seed = 318,
layers_DL = layers_DL, hidden_DL = hidden_DL,
epsilon_DL = epsilon_DL, rho_DL = rho_DL,
epochs_DL = epochs_DL,
mini_batch_size_DL = mini_batch_size_DL,
hidden_dropout_DL = hidden_dropout_DL,
max_runtime_secs_DL = max_runtime_secs_DL,
X = X.list[[4]], Y = ys[4], train = all.h2o, test = test.h2o,
distribution_DL = distribution_DL,
fast_mode_DL = fast_mode_DL,
activation_function_DL = activation_function_DL,
reproducible_DL = reproducible_DL,
nfolds_DL = nfolds_DL)
h2o.saveModel(DL_Tg_brooder_Ta_pen_all, "DL/Tg_brooder_Ta_pen/all")
h2o.saveMojo(DL_Tg_brooder_Ta_pen_all, "Best ML models/DL_Tg_brooder_Ta_pen_all.MOJO")
# DL Tr_Ta_pen
DL_Tr_Ta_pen = h2o.loadModel("Best ML models/DL_Tr_Ta_pen")
DL_Tr_Ta_pen_all = parallel.deep.learning(seed = 1930,
layers_DL = layers_DL, hidden_DL = hidden_DL,
epsilon_DL = epsilon_DL, rho_DL = rho_DL,
epochs_DL = epochs_DL,
mini_batch_size_DL = mini_batch_size_DL,
hidden_dropout_DL = hidden_dropout_DL,
max_runtime_secs_DL = max_runtime_secs_DL,
X = X.list[[5]], Y = ys[5], train = all.h2o, test = test.h2o,
distribution_DL = distribution_DL,
fast_mode_DL = fast_mode_DL,
activation_function_DL = activation_function_DL,
reproducible_DL = reproducible_DL,
nfolds_DL = nfolds_DL)
h2o.saveModel(DL_Tr_Ta_pen_all, "DL/Tr_Ta_pen/all")
h2o.saveMojo(DL_Tr_Ta_pen_all, "Best ML models/DL_Tr_Ta_pen_all.MOJO")
# DL Tr_Ta_pen
DL_Tr_Ta_brooder = h2o.loadModel("Best ML models/DL_Tr_Ta_brooder")
DL_Tr_Ta_brooder_all = parallel.deep.learning(seed = 1403,
layers_DL = layers_DL, hidden_DL = hidden_DL,
epsilon_DL = epsilon_DL, rho_DL = rho_DL,
epochs_DL = epochs_DL,
mini_batch_size_DL = mini_batch_size_DL,
hidden_dropout_DL = hidden_dropout_DL,
max_runtime_secs_DL = max_runtime_secs_DL,
X = X.list[[6]], Y = ys[6], train = all.h2o, test = test.h2o,
distribution_DL = distribution_DL,
fast_mode_DL = fast_mode_DL,
activation_function_DL = activation_function_DL,
reproducible_DL = reproducible_DL,
nfolds_DL = nfolds_DL)
h2o.saveModel(DL_Tr_Ta_brooder_all, "DL/Tr_Ta_brooder/all")
h2o.saveMojo(DL_Tr_Ta_brooder_all, "Best ML models/DL_Tr_Ta_brooder_all.MOJO")
print("load libraries")
library(gdata)
library(pbapply)
library(h2o)
library(foreach)
library(doMC)
library(pbmcapply)
library(foreign) # read.octave
setwd("~/OneDrive/Cornell/Publications/2018/Piglets 2013/code/V05/R/ML_training/Best ML models")
h2o.init()
# loading models
GLM_Ta_brooder = h2o.loadModel("GLM_Ta_brooder_all")
RF_Ta_brooder = h2o.loadModel("RF_Ta_brooder_all")
GBM_Ta_brooder = h2o.loadModel("GBM_Ta_brooder_all")
DL_Ta_brooder = h2o.loadModel("DL_Ta_brooder_all")
GLM_Tg_brooder = h2o.loadModel("GLM_Tg_brooder_all")
RF_Tg_brooder = h2o.loadModel("RF_Tg_brooder_all")
GBM_Tg_brooder = h2o.loadModel("GBM_Tg_brooder_all")
DL_Tg_brooder = h2o.loadModel("DL_Tg_brooder_all")
GLM_Tr = h2o.loadModel("GLM_Tr_all")
RF_Tr = h2o.loadModel("RF_Tr_all")
GBM_Tr = h2o.loadModel("GBM_Tr_all")
DL_Tr = h2o.loadModel("DL_Tr_all")
GLM_Tg_brooder_Ta_pen = h2o.loadModel("GLM_Tg_brooder_Ta_pen_all")
RF_Tg_brooder_Ta_pen = h2o.loadModel("RF_Tg_brooder_Ta_pen_all")
GBM_Tg_brooder_Ta_pen = h2o.loadModel("GBM_Tg_brooder_Ta_pen_all")
DL_Tg_brooder_Ta_pen = h2o.loadModel("DL_Tg_brooder_Ta_pen_all")
GLM_Tr_Ta_pen = h2o.loadModel("GLM_Tr_Ta_pen_all")
RF_Tr_Ta_pen = h2o.loadModel("RF_Tr_Ta_pen_all")
GBM_Tr_Ta_pen = h2o.loadModel("GBM_Tr_Ta_pen_all")
DL_Tr_Ta_pen = h2o.loadModel("DL_Tr_Ta_pen_all")
GLM_Tr_Ta_brooder = h2o.loadModel("GLM_Tr_Ta_brooder_all")
RF_Tr_Ta_brooder = h2o.loadModel("RF_Tr_Ta_brooder_all")
GBM_Tr_Ta_brooder = h2o.loadModel("GBM_Tr_Ta_brooder_all")
DL_Tr_Ta_brooder = h2o.loadModel("DL_Tr_Ta_brooder_all")
# loading LM models
load("../LM/LM_models_new.RData")
# loading full data
data_piglets = read.csv("../../../datasets/data_piglets.csv")
data_piglets_training = read.octave("../../../datasets/trainingPosition.mat")
data_piglets_training = as.vector(data_piglets_training$trainingPosition)
data_piglets_testing = read.octave("../../../datasets/testingPosition.mat")
data_piglets_testing = as.vector(data_piglets_testing$testingPosition)
all.dataset = data_piglets[c(data_piglets_training, data_piglets_testing),] # nrow(train.dataset) # 130
# data for ploting boxes
Ta_pen_min = min(all.dataset$Ta_pen)
Ta_pen_max = max(all.dataset$Ta_pen)
Ta_brooder_min = min(all.dataset$Ta_brooder)
Ta_brooder_max = max(all.dataset$Ta_brooder)
Tg_brooder_min = min(all.dataset$Tg_brooder)
Tg_brooder_max = max(all.dataset$Tg_brooder)
heat_power_min = min(all.dataset$Heat_source_power)
heat_power_max = max(all.dataset$Heat_source_power)
